from __future__ import annotations

"""Generate follow-up questions for Pops Analytics Copilot conversations.

This helper is deliberately lightweight so it can be called inside the main
/chat endpoint without adding noticeable latency.  Use a cheap, fast model by
default and keep the prompt short.
"""

import os
from typing import List

import openai
from dotenv import load_dotenv

load_dotenv()
openai.api_key = os.getenv("OPENAI_API_KEY")

_DEFAULT_MODEL = os.getenv("COPILOT_SUGGESTION_MODEL", "gpt-3.5-turbo-0125")

_PROMPT_TMPL = (
    "You are an analytics conversation assistant. Given the user's original "
    "question and the assistant's answer, propose three concise and useful "
    "follow-up questions the user might want to ask next. "
    "Respond with each question on a new line WITHOUT numbering or bullets."
)


def generate_suggestions(question: str, answer: str, model: str | None = None) -> List[str]:
    """Return up to three follow-up questions generated by the LLM.

    Falls back to a static heuristic in case of API errors or missing creds.
    """
    model_name = model or _DEFAULT_MODEL

    try:
        resp = openai.chat.completions.create(
            model=model_name,
            messages=[
                {"role": "system", "content": _PROMPT_TMPL},
                {"role": "user", "content": f"Question: {question}\nAnswer: {answer}"},
            ],
            temperature=0.3,
            max_tokens=60,
        )
        text = resp.choices[0].message.content.strip()
        # Split by newlines, strip empty lines.
        suggestions = [ln.strip("-â€¢ ").strip() for ln in text.split("\n") if ln.strip()]
        # Cap at 3.
        if len(suggestions) > 3:
            suggestions = suggestions[:3]
        # Ensure they end with a question mark; if not, append.
        suggestions = [s if s.endswith("?") else f"{s}?" for s in suggestions]
        # Basic sanity check.
        if all(5 < len(s) < 120 for s in suggestions):
            return suggestions
    except Exception:
        # Swallow and fall back below.
        pass

    # ---------------------------------------------------------------------
    # Fallback static list (matches previous behaviour)
    # ---------------------------------------------------------------------
    return [
        "Could you break this down by top traffic source?",
        "What specific actions should we prioritise next week?",
        "How might seasonality be affecting these numbers?",
    ] 