import os
import glob
import pandas as pd
from pathlib import Path
from collections import defaultdict
from textwrap import indent, dedent

"""
variable_catalog_generator.py
─────────────────────────────

Utility to scan the parquet files produced by the three extractors (GA4, Google Ads,
Search Console) and emit a Markdown table describing the columns, their pandas dtype/
formats, and example or enumerated values.

Run from repo root:
    PYTHONPATH=. python copilot/utils/variable_catalog_generator.py > copilot/variable_catalog.md

If the data repo contains multiple months, the script only samples the first parquet
file it finds per data-source.  Adjust `SAMPLES_PER_SOURCE` below to include more.
"""

DATA_SOURCES = {
    "GA4": {
        "glob": "data_repo/ga4/analytics_events_final/report_month=*/**/*.parquet",
        "description": "Google Analytics 4 events exported via BigQuery",
    },
    "Google Ads": {
        "glob": "data_repo/google_ads/google_ads_final/report_month=*/**/*.parquet",
        "description": "Google Ads keyword-level performance data via Google Ads API",
    },
    "Search Console": {
        "glob": "data_repo/search_console/search_console_final/report_month=*/**/*.parquet",
        "description": "Google Search Console search-analytics data",
    },
}

SAMPLES_PER_SOURCE = 1  # number of parquet files to sample per source
UNIQUE_THRESHOLD = 20    # if unique values <= threshold, enumerate them
SAMPLE_VALUES = 5        # otherwise, sample a few example values

def summarise_series(s: pd.Series):
    """Return dtype string and example / enum list for the series."""
    dtype = str(s.dtype)
    non_null = s.dropna()
    if non_null.empty:
        return dtype, "all null"

    unique_count = non_null.nunique(dropna=True)

    # Enumerate discrete small sets
    if unique_count <= UNIQUE_THRESHOLD:
        unique_vals = sorted(non_null.unique())
        return dtype, ", ".join(map(str, unique_vals))

    # Otherwise give sample values / statistics
    if pd.api.types.is_numeric_dtype(s):
        return dtype, f"min={non_null.min()}, max={non_null.max()}"

    if pd.api.types.is_datetime64_any_dtype(s):
        return dtype, f"min={non_null.min().date()}, max={non_null.max().date()}"

    # generic object/string
    sample_vals = non_null.sample(min(SAMPLE_VALUES, len(non_null)), random_state=0).unique()
    return dtype, "; ".join(map(str, sample_vals))


def inspect_parquet(path: str):
    """Load a parquet file into a DataFrame (only first 10k rows to save memory)."""
    try:
        df = pd.read_parquet(path, engine="pyarrow")
        if len(df) > 10_000:
            df = df.head(10_000)
        return df
    except Exception as e:
        print(f"⚠️  Failed to read {path}: {e}")
        return pd.DataFrame()


def main():
    markdown_lines = [
        "# POPS Analytics – Data Extractor Variable Catalog\n",
        "Automatically generated by `variable_catalog_generator.py`.  For each data-source \n",
        "extractor we list all columns observed in the sampled parquet files, their pandas \n",
        "dtype / format as loaded by `pandas.read_parquet`, and either a set of enumerated \n",
        "values (if the cardinality is small) or a short statistical/sample summary.\n",
        "\n",
        "_Regenerate this file whenever the schema changes:_\n",
        "```bash\n",
        "PYTHONPATH=. python copilot/utils/variable_catalog_generator.py > copilot/variable_catalog.md\n",
        "```\n",
        "\n",
    ]

    for source, cfg in DATA_SOURCES.items():
        parquet_files = glob.glob(cfg["glob"], recursive=True)
        parquet_files.sort()
        sample_paths = parquet_files[:SAMPLES_PER_SOURCE]

        markdown_lines.append("\n---\n")
        markdown_lines.append(f"\n## {source}\n")
        markdown_lines.append(f"Data description: {cfg['description']}\n")
        if not sample_paths:
            markdown_lines.append("\n_No parquet files found. Run the extractor first._\n")
            continue

        combined_df = pd.concat(
            [inspect_parquet(p) for p in sample_paths], axis=0, ignore_index=True, sort=False
        )

        if combined_df.empty:
            markdown_lines.append("\n_Failed to load sample parquet data._\n")
            continue

        markdown_lines.append("\n| Column | dtype / format | Example / enumerated values |\n")
        markdown_lines.append("|--------|----------------|-----------------------------|\n")
        for col in combined_df.columns:
            dtype, examples = summarise_series(combined_df[col])
            examples = examples.replace("|", "\|")  # escape pipe for markdown
            markdown_lines.append(f"| `{col}` | {dtype} | {examples} |\n")

    output = "".join(markdown_lines)
    print(output)


if __name__ == "__main__":
    main() 